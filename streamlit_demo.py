import os # é€™å€‹æ˜¯ Python çš„ os æ¨¡çµ„ï¼Œç”¨æ–¼ç”Ÿæˆ os
import streamlit as st
import shutil # é€™å€‹æ˜¯ Python çš„ shutil æ¨¡çµ„ï¼Œç”¨æ–¼ç”Ÿæˆ shutil
from transformers import AutoTokenizer
from langchain_core.output_parsers import StrOutputParser # é€™å€‹æ˜¯ LangChain çš„ StrOutputParserï¼Œç”¨æ–¼ç”Ÿæˆ StrOutputParser
from langchain.memory import ConversationBufferMemory # é€™å€‹æ˜¯ LangChain çš„ ConversationBufferMemoryï¼Œç”¨æ–¼ç”Ÿæˆ ConversationBufferMemory
from core import ( # é€™å€‹æ˜¯ core æ¨¡çµ„çš„ Configã€load_documentã€split_into_chunksã€get_embeddingã€create_embedding_chromaã€build_retrieverã€load_llmã€rag_chain
    Config, 
    load_document, 
    split_into_chunks, 
    get_embedding,
    create_embedding_chroma,
    build_retriever,
    load_llm,
    rag_chain
)

# intialize session state
if 'messages' not in st.session_state: # å¦‚æœ 'messages' ä¸åœ¨ st.session_state ä¸­ï¼Œå‰‡åˆå§‹åŒ– st.session_state.messages
    st.session_state.messages = []
if 'chat_memory' not in st.session_state: # å¦‚æœ 'chat_memory' ä¸åœ¨ st.session_state ä¸­ï¼Œå‰‡åˆå§‹åŒ– st.session_state.chat_memory
    st.session_state.chat_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True) # ä½¿ç”¨ ConversationBufferMemory åˆå§‹åŒ– st.session_state.chat_memory
if 'vs' not in st.session_state: # å¦‚æœ 'vs' ä¸åœ¨ st.session_state ä¸­ï¼Œå‰‡åˆå§‹åŒ– st.session_state.vs
    st.session_state.vs = None # åˆå§‹åŒ– st.session_state.vs

# calculate embedding cost using tikoken
def calculate_embedding_token(embedding_model_name: str, texts): # é€™å€‹æ˜¯ calculate_embedding_token å‡½æ•¸ï¼Œç”¨æ–¼ç”Ÿæˆ calculate_embedding_token
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name) # ä½¿ç”¨ AutoTokenizer åˆå§‹åŒ– tokenizer
    total_tokens = sum(len(tokenizer.encode(page.page_content)) for page in texts) # ä½¿ç”¨ tokenizer ç·¨ç¢¼ texts
    return total_tokens # è¿”å›ç·¨ç¢¼å¾Œçš„ token æ•¸é‡

# remove historyï¼ˆmemory, messages, vector storeï¼‰
def clear_history(): # é€™å€‹æ˜¯ clear_history å‡½æ•¸ï¼Œç”¨æ–¼ç”Ÿæˆ clear_history
    st.session_state.messages = [] # æ¸…ç©º st.session_state.messages
    st.session_state.chat_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True) # ä½¿ç”¨ ConversationBufferMemory åˆå§‹åŒ– st.session_state.chat_memory
    # Properly delete vector store
    if st.session_state.vs is not None: # å¦‚æœ st.session_state.vs ä¸æ˜¯ Noneï¼Œå‰‡æ¸…ç©º st.session_state.vs
        try:
            del st.session_state.vs # æ¸…ç©º st.session_state.vs
        except:
            pass # å¦‚æœæ¸…ç©º st.session_state.vs å¤±æ•—ï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
    st.session_state.vs = None
    
# clear_chroma_db
def clear_chroma_db(path='./knowledge_db'): # é€™å€‹æ˜¯ clear_chroma_db å‡½æ•¸ï¼Œç”¨æ–¼ç”Ÿæˆ clear_chroma_db
    """æ¸…ç©º Chroma æœ¬åœ°åµŒå…¥è³‡æ–™è³‡æ–™åº«ç›®éŒ„"""
    if os.path.isdir(path): # å¦‚æœ path æ˜¯ç›®éŒ„ï¼Œå‰‡æ¸…ç©º path
        shutil.rmtree(path) # æ¸…ç©º path
        print(f"Chroma å‘é‡è³‡æ–™åº«ä½æ–¼ {path} å·²è¢«æ¸…ç©ºã€‚") # å¦‚æœ path æ˜¯ç›®éŒ„ï¼Œå‰‡æ¸…ç©º path
    else: # å¦‚æœ path ä¸æ˜¯ç›®éŒ„ï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        print(f"æ‰¾ä¸åˆ° {path} ç›®éŒ„ï¼Œä¸éœ€æ¸…ç©ºã€‚") # å¦‚æœ path ä¸æ˜¯ç›®éŒ„ï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ


# ask and get answer method (normal output)
def ask_then_get_answer(vector_store, query, top_k): # é€™å€‹æ˜¯ ask_then_get_answer å‡½æ•¸ï¼Œç”¨æ–¼ç”Ÿæˆ ask_then_get_answer
    import time
    from google.api_core import exceptions as google_exceptions # é€™å€‹æ˜¯ Google API çš„ exceptions æ¨¡çµ„ï¼Œç”¨æ–¼ç”Ÿæˆ google_exceptions
    
    try: # å¦‚æœæ¸…ç©º st.session_state.vs å¤±æ•—ï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        llm = load_llm(model_name=Config.MODEL_NAME, model_provider=Config.MODEL_PROVIDER, temperature=1) # ä½¿ç”¨ load_llm åˆå§‹åŒ– llm
        retriever = build_retriever(vector_store=vector_store, top_k=top_k) # ä½¿ç”¨ build_retriever åˆå§‹åŒ– retriever
        memory = st.session_state.chat_memory # ä½¿ç”¨ st.session_state.chat_memory åˆå§‹åŒ– memory
        chain = rag_chain(retriever=retriever, llm=llm, memory=None)
        chat_input = {
            'query': query,
            'chat_history': memory.load_memory_variables({})['chat_history'] # ä½¿ç”¨ memory.load_memory_variables åˆå§‹åŒ– chat_input
        }
        answer = chain.invoke(chat_input) # ä½¿ç”¨ chain.invoke åˆå§‹åŒ– answer
        return answer # è¿”å› answer
    except google_exceptions.ResourceExhausted as e: # å¦‚æœ google_exceptions.ResourceExhausted ç™¼ç”Ÿï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        error_msg = str(e)
        if "quota" in error_msg.lower() or "429" in error_msg:
            wait_time = 20  # Default wait 20 seconds # é»˜èªç­‰å¾… 20 ç§’
            # Try to extract wait time from error message
            if "retry in" in error_msg.lower():
                import re
                match = re.search(r'retry in ([\d.]+)s', error_msg.lower()) # ä½¿ç”¨ re.search åˆå§‹åŒ– match
                if match:
                    wait_time = int(float(match.group(1))) + 5  # Add 5 seconds buffer
            
            st.error(f"""
            âš ï¸ **API Quota Exceeded**
            
            Google Gemini API free tier has a daily limit of **20 requests**, and you have reached this limit.
            
            **Solutions:**
            1. â° Wait approximately {wait_time} seconds and try again
            2. ğŸ’³ Go to [Google Cloud Console](https://console.cloud.google.com/) to set up billing for higher quota
            3. ğŸ”„ Try again tomorrow (quota resets daily)
            
            **More Information:** [Gemini API Quota Guide](https://ai.google.dev/gemini-api/docs/rate-limits)
            """)
            return None
        else:
            raise
    except Exception as e: # å¦‚æœ Exception ç™¼ç”Ÿï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        st.error(f"âŒ An error occurred: {str(e)}") # ä½¿ç”¨ st.error åˆå§‹åŒ– error
        return None # è¿”å› None

# Streaming answer method
def ask_then_get_answer_streaming(vector_store, query, top_k):
    from google.api_core import exceptions as google_exceptions
    
    try: # å¦‚æœæ¸…ç©º st.session_state.vs å¤±æ•—ï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        llm = load_llm(model_name=Config.MODEL_NAME, model_provider=Config.MODEL_PROVIDER, temperature=1) # ä½¿ç”¨ load_llm åˆå§‹åŒ– llm
        retriever = build_retriever(vector_store=vector_store, top_k=top_k) # ä½¿ç”¨ build_retriever åˆå§‹åŒ– retriever
        memory = st.session_state.chat_memory # ä½¿ç”¨ st.session_state.chat_memory åˆå§‹åŒ– memory
        chain = rag_chain(retriever=retriever, llm=llm, memory=None) # ä½¿ç”¨ rag_chain åˆå§‹åŒ– chain
        chat_input = {
            'query': query,
            'chat_history': memory.load_memory_variables({})['chat_history'] # ä½¿ç”¨ memory.load_memory_variables åˆå§‹åŒ– chat_input
        }
        
        # Stream the response
        full_response = "" # åˆå§‹åŒ– full_response
        for chunk in chain.stream(chat_input): # ä½¿ç”¨ chain.stream åˆå§‹åŒ– chunk
            if hasattr(chunk, 'content'):
                full_response += chunk.content
                yield chunk.content # è¿”å› chunk.content
            else:
                content = str(chunk) # ä½¿ç”¨ str åˆå§‹åŒ– content
                full_response += content # ä½¿ç”¨ full_response åˆå§‹åŒ– full_response
                yield content # è¿”å› content
        
        return full_response # è¿”å› full_response
        
    except google_exceptions.ResourceExhausted as e: # å¦‚æœ google_exceptions.ResourceExhausted ç™¼ç”Ÿï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        error_msg = str(e)
        if "quota" in error_msg.lower() or "429" in error_msg:
            wait_time = 20 # é»˜èªç­‰å¾… 20 ç§’
            if "retry in" in error_msg.lower():
                import re
                match = re.search(r'retry in ([\d.]+)s', error_msg.lower()) # ä½¿ç”¨ re.search åˆå§‹åŒ– match
                if match:
                    wait_time = int(float(match.group(1))) + 5 # ä½¿ç”¨ int åˆå§‹åŒ– wait_time
            
            st.error(f"""
            âš ï¸ **API Quota Exceeded**
            
            Google Gemini API free tier has a daily limit of **20 requests**, and you have reached this limit.
            
            **Solutions:**
            1. â° Wait approximately {wait_time} seconds and try again
            2. ğŸ’³ Go to [Google Cloud Console](https://console.cloud.google.com/) to set up billing for higher quota
            3. ğŸ”„ Try again tomorrow (quota resets daily)
            
            **More Information:** [Gemini API Quota Guide](https://ai.google.dev/gemini-api/docs/rate-limits)
            """)
            return None # è¿”å› None
        else:
            raise # å¦‚æœ Exception ç™¼ç”Ÿï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
    except Exception as e: # å¦‚æœ Exception ç™¼ç”Ÿï¼Œå‰‡ä¸é€²è¡Œä»»ä½•æ“ä½œ
        st.error(f"âŒ An error occurred: {str(e)}") # ä½¿ç”¨ st.error åˆå§‹åŒ– error
        return None # è¿”å› None

if __name__ == "__main__": # å¦‚æœ __name__ æ˜¯ "__main__"ï¼Œå‰‡åŸ·è¡Œä»¥ä¸‹ç¨‹å¼
    
    # Page configuration
    st.set_page_config( # ä½¿ç”¨ st.set_page_config åˆå§‹åŒ–é é¢é…ç½®
        page_title="RAG Question Answering System", # ä½¿ç”¨ page_title åˆå§‹åŒ–é é¢æ¨™é¡Œ
        page_icon="ğŸ¤–", # ä½¿ç”¨ page_icon åˆå§‹åŒ–é é¢åœ–ç¤º
        layout="wide", # ä½¿ç”¨ layout åˆå§‹åŒ–é é¢å¸ƒå±€
        initial_sidebar_state="expanded" # ä½¿ç”¨ initial_sidebar_state åˆå§‹åŒ–é é¢å´é‚Šæ¬„ç‹€æ…‹
    )
    
    # Main title area
    col1, col2 = st.columns([3, 1]) # ä½¿ç”¨ st.columns åˆå§‹åŒ– col1 å’Œ col2
    with col1:
        st.title('ğŸ¤– RAG Question Answering System') # ä½¿ç”¨ st.title åˆå§‹åŒ–é é¢æ¨™é¡Œ
        st.markdown('**Knowledge Base Q&A System based on Retrieval-Augmented Generation (RAG)**') # ä½¿ç”¨ st.markdown åˆå§‹åŒ–é é¢å…§å®¹
    with col2:
        if st.session_state.vs is not None: # å¦‚æœ st.session_state.vs ä¸æ˜¯ Noneï¼Œå‰‡ä½¿ç”¨ st.success åˆå§‹åŒ–é é¢ç‹€æ…‹
            st.success('âœ… Vector Store Ready', icon="ğŸ“š") # ä½¿ç”¨ st.success åˆå§‹åŒ–é é¢ç‹€æ…‹
        else: # å¦‚æœ st.session_state.vs æ˜¯ Noneï¼Œå‰‡ä½¿ç”¨ st.info åˆå§‹åŒ–é é¢ç‹€æ…‹
            st.info('â³ Waiting for File Upload', icon="ğŸ“¦") # ä½¿ç”¨ st.info åˆå§‹åŒ–é é¢ç‹€æ…‹
    
    st.divider() # ä½¿ç”¨ st.divider åˆå§‹åŒ–é é¢åˆ†å‰²ç·š
    
    with st.sidebar:
        st.markdown(
            "<h3 style='text-align: center; margin-bottom: 0;'>RAG FILE SYSTEM CONSOLE</h3>", # ä½¿ç”¨ st.markdown åˆå§‹åŒ–é é¢å…§å®¹
            unsafe_allow_html=True, # ä½¿ç”¨ unsafe_allow_html åˆå§‹åŒ–é é¢å…§å®¹
        )
        
        st.divider() # ä½¿ç”¨ st.divider åˆå§‹åŒ–é é¢åˆ†å‰²ç·š
        
        # API Configuration
        with st.expander("ğŸ”‘ API Configuration", expanded=True): # ä½¿ç”¨ st.expander åˆå§‹åŒ–é é¢å…§å®¹  
            api_key = st.text_input(
                label='Google API Key', 
                type='password',
                help='Enter your Google Gemini API Key',
                placeholder='Enter your API Key...'
            )
            if api_key:
                os.environ['GOOGLE_API_KEY'] = api_key
                st.success('âœ… API Key Set')
        
        # Model Configuration
        with st.expander("âš™ï¸ Model Settings", expanded=False):
            llm = st.selectbox(
                label='Chat Model',
                options=['gemini-2.5-flash'],
                help='Select the language model to use'
            )
            
            vector_store_option = st.selectbox(
                label='Vector Database',
                options=['Chroma'],
                help='Select vector database type'
            )
            
            output_type = st.selectbox(
                label='Output Mode', 
                options=['Normal Output', 'Streaming Output'],
                help='Select response output mode'
            )
        
        # File Upload
        with st.expander("ğŸ“„ Document Management", expanded=True):
            upload_file = st.file_uploader(
                'Upload Knowledge Base Document', 
                type=['pdf', 'docx', 'txt', 'markdown'],
                help='Supports PDF, DOCX, TXT, Markdown formats, max 200MB'
            )
            
            if upload_file:
                file_size = len(upload_file.getvalue()) / 1024  # KB
                st.caption(f"ğŸ“ {upload_file.name} ({file_size:.1f} KB)")
        
        # Advanced Parameters
        with st.expander("ğŸ”§ Advanced Parameters", expanded=False):
            chunk_size = st.number_input(
                'Chunk Size', 
                min_value=100, 
                max_value=2048, 
                value=500,
                step=50,
                help='Size of text chunks for splitting',
                on_change=clear_history
            )
            
            chunk_overlap = st.number_input(
                'Chunk Overlap', 
                min_value=0, 
                max_value=512, 
                value=2,
                step=1,
                help='Number of overlapping characters between chunks',
                on_change=clear_history
            )
            
            k = st.number_input(
                'Top K', 
                min_value=1, 
                max_value=20, 
                value=3,
                step=1,
                help='Number of most relevant document chunks to retrieve',
                on_change=clear_history
            )
        
        st.divider()
        
        # Action Buttons
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            add_data = st.button('Add Data', use_container_width=True, type='primary')
        with col_btn2:
            if st.button('Clear All', use_container_width=True, on_click=clear_history):
                st.rerun()
        
        # Status Display
        if st.session_state.vs is not None:
            st.success('âœ… Knowledge Base Loaded', icon="ğŸ“š")
        else:
            st.info('Please upload and add a file first', icon="ğŸ“")

        # File Processing
        if upload_file and add_data:
            # Clear old vector store before processing new file
            if st.session_state.vs is not None:
                try:
                    # Try to delete the collection if it exists
                    from langchain_chroma import Chroma
                    if hasattr(st.session_state.vs, '_collection') and st.session_state.vs._collection is not None:
                        try:
                            # Delete the collection
                            st.session_state.vs._client.delete_collection(st.session_state.vs._collection.name)
                        except:
                            pass
                    # Delete the old vector store object
                    del st.session_state.vs
                except:
                    pass
                st.session_state.vs = None
            
            # Clear chat history when uploading new file
            st.session_state.messages = []
            st.session_state.chat_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # Step 1: Load document
                status_text.info('ğŸ“– Loading document...')
                progress_bar.progress(10)
                bytes_data = upload_file.read()
                file_name = os.path.join('./', upload_file.name)
                with open(file_name, 'wb') as f:
                    f.write(bytes_data)
                data = load_document(file_name)
                
                # Step 2: Split text
                status_text.info('âœ‚ï¸ Splitting text...')
                progress_bar.progress(30)
                chunks = split_into_chunks(data, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                
                # Step 3: Calculate tokens
                status_text.info('ğŸ”¢ Calculating tokens...')
                progress_bar.progress(50)
                total_tokens = calculate_embedding_token(Config.EMBEDDING_MODEL_NAME, chunks)
                
                # Step 4: Generate embeddings
                status_text.info('ğŸ§® Generating vector embeddings...')
                progress_bar.progress(70)
                embedding = get_embedding(Config.EMBEDDING_MODEL_NAME)
                
                # Step 5: Create NEW vector store with unique collection name
                status_text.info('ğŸ’¾ Creating vector database...')
                progress_bar.progress(90)
                if vector_store_option == 'Chroma':
                    # Create a completely new vector store with unique collection name
                    import uuid
                    collection_name = f"doc_{uuid.uuid4().hex[:8]}"
                    vector_store_obj = create_embedding_chroma(chunks, embedding, collection_name=collection_name)
                    st.session_state.vs = vector_store_obj
                
                # Complete
                progress_bar.progress(100)
                status_text.empty()
                progress_bar.empty()
                
                # Display results
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Document Chunks", len(chunks))
                with col2:
                    st.metric("Chunk Size", chunk_size)
                with col3:
                    st.metric("Tokens", f"{total_tokens:,}")
                
                st.success(f'âœ… Document "{upload_file.name}" has been successfully processed and loaded into the knowledge base!')
                
                # Clean up temporary file
                os.remove(file_name)
                
            except Exception as e:
                status_text.empty()
                progress_bar.empty()
                st.error(f'âŒ Error processing file: {str(e)}')
    
    # Main chat area
    if st.session_state.vs is None:
        st.info('ğŸ’¡ **Tip**: Please upload a file in the sidebar and click "Add Data" to initialize the knowledge base', icon='ğŸ“¦')
        st.markdown("""
        ### ğŸ“‹ Usage Steps:
        1. **Upload Document**: Select a PDF, DOCX, TXT, or Markdown file in the sidebar
        2. **Set Parameters**: Adjust Chunk Size, Overlap, and Top K (optional)
        3. **Add Data**: Click the "Add Data" button and wait for processing to complete
        4. **Start Chatting**: Enter your question in the input box below to start chatting with the knowledge base
        """)    
    
    # Chat history display
    if st.session_state.messages:
        st.markdown("### ğŸ’¬ Chat History")
        for msg in st.session_state.messages:
            with st.chat_message(msg['role']):
                st.markdown(msg['content'])
    else:
        # Welcome message
        st.markdown("### ğŸ‘‹ Welcome to RAG Question Answering System")
        st.markdown("""
        This is an intelligent Q&A system based on **Retrieval-Augmented Generation (RAG)** technology that can:
        
        - ğŸ“š **Knowledge Retrieval**: Accurately retrieve relevant information from your uploaded documents
        - ğŸ¤– **Intelligent Answers**: Generate accurate, well-grounded answers using large language models
        - ğŸ’­ **Context Memory**: Support multi-turn conversations with context understanding
        
        **Get Started**: Enter your question in the input box below ğŸ‘‡
        """)
            
    # User input
    prompt = st.chat_input(placeholder='Enter your question...')
    if prompt:
        vector_store = st.session_state.vs   # <--- Must be here
        if vector_store is None:
            st.warning('Vector store not initialized. Please upload and embed a file first!')
            st.stop()
        # User input
        with st.chat_message('user'):
            st.markdown(prompt)
        st.session_state.messages.append({'role': 'user', 'content': prompt})

        # Assistant response
        response = None
        response_content = ""
        
        if output_type == 'Normal Output':
            if llm == 'gemini-2.5-flash':
                response = ask_then_get_answer(vector_store, prompt, k)
            else:
                response = "Please select a valid model."
            
            if response is not None:
                with st.chat_message('assistant'):
                    if hasattr(response, 'content'):
                        st.markdown(response.content)
                        response_content = response.content
                    else:
                        st.markdown(str(response))
                        response_content = str(response)
                st.session_state.messages.append({'role': 'assistant', 'content': response_content})

                # Sync conversation to LangChain memory
                st.session_state.chat_memory.save_context(
                    {"input": prompt},
                    {"output": response_content}
                )
        
        elif output_type == 'Streaming Output':
            if llm == 'gemini-2.5-flash':
                with st.chat_message('assistant'):
                    # Create a placeholder for streaming text
                    message_placeholder = st.empty()
                    full_response = ""
                    
                    # Stream the response
                    try:
                        for chunk in ask_then_get_answer_streaming(vector_store, prompt, k):
                            if chunk:
                                full_response += chunk
                                # Update the placeholder with the current response
                                message_placeholder.markdown(full_response + "â–Œ")
                        
                        # Final update without cursor
                        message_placeholder.markdown(full_response)
                        response_content = full_response
                        
                    except Exception as e:
                        st.error(f"âŒ Streaming error: {str(e)}")
                        response_content = ""
                
                if response_content:
                    st.session_state.messages.append({'role': 'assistant', 'content': response_content})
                    
                    # Sync conversation to LangChain memory
                    st.session_state.chat_memory.save_context(
                        {"input": prompt},
                        {"output": response_content}
                    )
            else:
                with st.chat_message('assistant'):
                    st.markdown("Please select a valid model.")
                st.session_state.messages.append({'role': 'assistant', 'content': "Please select a valid model."})